{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\">Complex Networks - Practical Session 2</h1>\n",
    "<h3 style=\"text-align: center;\" markdown=\"1\">by Dimitri Lajou and Fabrice Lebeau</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we implement and illustrates two methods of constructing random graphs: the **Erdös-Rényi random graph model** and the **configuration model of random networks**. \n",
    "\n",
    "Let us first import the tools we are going to use and our own `Graph` class that we began to define in the last practical session. \n",
    "\n",
    "**Important note:** in order for our examples to be more interactive, we are using `ipywidgets`. You may have to install this module, e.g. with `pip`:\n",
    "\n",
    "    pip install ipywidgets\n",
    "\n",
    "Moreover, you may need to allow this extension for Jupyter by executing the following command in a terminal:\n",
    "    \n",
    "    jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import scipy\n",
    "import scipy.special\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt;\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "plt.rcParams['font.size'] = 14\n",
    "from IPython.display import Math, Markdown, Latex, display, display_latex, SVG\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" CR15 graph library \"\"\"\n",
    "class Graph(object):\n",
    "\n",
    "    def __init__(self, graph_dict={}, graph_name=\"\"):\n",
    "        \"\"\" initializes a graph object \"\"\"\n",
    "        self.__graph_dict = graph_dict.copy()\n",
    "        self.__name=graph_name\n",
    "\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "        \n",
    "    def vertices(self):\n",
    "        \"\"\" returns the vertices of a graph \"\"\"\n",
    "        return list(self.__graph_dict.keys())\n",
    "\n",
    "    def edges(self):\n",
    "        \"\"\" returns the edges of a graph \"\"\"\n",
    "        return self.__generate_edges()\n",
    "    \n",
    "    def connected_components(self):\n",
    "        return self.__generate_components()\n",
    "\n",
    "    def add_vertex(self, vertex):\n",
    "        \"\"\" If vertex is not in self.__graph_dict, a key \"vertex\" with an empty\n",
    "        list as a value is added to the dictionary. Otherwise nothing has to be \n",
    "        done.\"\"\"\n",
    "        if not vertex in self.__graph_dict:\n",
    "            self.__graph_dict[vertex] = []\n",
    "        \n",
    "\n",
    "    def add_edge(self, edge):\n",
    "        \"\"\" assumes that edge is of type set, tuple or list. No loops or \n",
    "        multiple edges.\"\"\"\n",
    "        my_edge = list(edge)\n",
    "        if len(my_edge) != 2: raise WrongSizeForEdge()\n",
    "        u = edge.pop()\n",
    "        v = edge.pop()\n",
    "        if u in self.__graph_dict and v in self.__graph_dict:\n",
    "            if u != v:\n",
    "                if v not in self.__graph_dict[u]:\n",
    "                    self.__graph_dict[u].append(v)\n",
    "                if u not in self.__graph_dict[v]:\n",
    "                    self.__graph_dict[v].append(u)\n",
    "        else:\n",
    "            raise VerticesNotDecleared()\n",
    "            \n",
    "\n",
    "    def __generate_edges(self):\n",
    "        \"\"\" A static method generating the edges of the graph \"graph\". Edges \n",
    "        are represented as sets two vertices, with no loops. To complete.\"\"\"\n",
    "        edges = []\n",
    "        for v, edges_list in self.__graph_dict.items():\n",
    "            for u in edges_list:\n",
    "                if v < u:\n",
    "                    edges.append(set([v,u]))\n",
    "        return edges\n",
    "    \n",
    "    def vertex_degree(self, vertex):\n",
    "        \"\"\"Return a dictionary degree\"\"\"\n",
    "        return len(self.__graph_dict[vertex])\n",
    "    \n",
    "    def vertex_degrees(self):\n",
    "        \"\"\"Return a dictionary degree\"\"\"\n",
    "        degrees = {}\n",
    "        for v, edges_list in self.__graph_dict.items():\n",
    "            degrees[v] = len(edges_list)\n",
    "        return degrees\n",
    "    \n",
    "    def vertex_degree_simple(self, vertex):\n",
    "        \"\"\"Return a dictionary degree\"\"\"\n",
    "        edges = set(self.__graph_dict[vertex])\n",
    "        if vertex in edges:\n",
    "            return len(edges) - 1\n",
    "        else:\n",
    "            return len(edges)    \n",
    "            \n",
    "    def vertex_degrees_simple(self):\n",
    "        \"\"\"Return a dictionary degree\"\"\"\n",
    "        degrees = {}\n",
    "        for v in self.__graph_dict:\n",
    "            degrees[v] = self.vertex_degree_simple(v)\n",
    "        return degrees\n",
    "    \n",
    "    def degree_distrib_simple(self):\n",
    "        distrib = {}\n",
    "        for v in self.__graph_dict:\n",
    "            k = self.vertex_degree_simple(v)\n",
    "            if k in distrib:\n",
    "                distrib[k] += 1\n",
    "            else:\n",
    "                distrib[k] = 1\n",
    "        for k in distrib:\n",
    "            distrib[k] /= len(self.__graph_dict)\n",
    "        return distrib\n",
    "    \n",
    "    \n",
    "    def find_isolated_vertices(self):\n",
    "        \"\"\"Return a set of zero-degree verticies\"\"\"\n",
    "        zero_set = set()\n",
    "        for v, edges_list in self.__graph_dict.items():\n",
    "            if len(edges_list) == 0:\n",
    "                zero_set.add(v)\n",
    "        return zero_set\n",
    "    \n",
    "    def has_isolated_vertices(self):\n",
    "        \"\"\"Return a set of zero-degree verticies\"\"\"\n",
    "        for v, edges_list in self.__graph_dict.items():\n",
    "            if len(edges_list) == 0:\n",
    "                return True\n",
    "        return False\n",
    "                \n",
    "    def density(self):\n",
    "        \"\"\"Return the density of the graph\"\"\"\n",
    "        deg = self.vertex_degrees()\n",
    "        density = 0\n",
    "        for v, d in deg.items():\n",
    "            density += d\n",
    "        density /=  (len(deg) -1) *len(deg)\n",
    "        return density\n",
    "                    \n",
    "    def dict(self):\n",
    "        return self.__graph_dict\n",
    "    \n",
    "    def degree_sequence(self):\n",
    "        \"\"\"Return the list of vertex degree sorted by decreasing degree\"\"\"\n",
    "        deg = self.vertex_degrees()\n",
    "        deg_list = [v for v in deg.values()]\n",
    "        deg_list.sort(reverse=True)\n",
    "        return tuple(deg_list)\n",
    "    \n",
    "    @staticmethod\n",
    "    def erdos_gallai(deg_seq):\n",
    "        \"\"\"Given a degree sequence, this method verify that this sequence verify the erdos gallai conditions\"\"\"\n",
    "        even_number = 0\n",
    "        for v in deg_seq:\n",
    "            even_number += v\n",
    "        if even_number % 2 == 1 :\n",
    "            return False\n",
    "        sumOfdi = 0\n",
    "        for k in range(len(deg_seq)):\n",
    "            sumOfdi += deg_seq[k]\n",
    "            sumOfMin = k*(k+1)\n",
    "            for i in range(k, len(deg_seq)):\n",
    "                sumOfMin += min(deg_seq[i], k+1)\n",
    "            if sumOfMin < sumOfdi:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def global_clustering_coefficient(self):\n",
    "        triangle = 0\n",
    "        triplet = 0\n",
    "        for v in self.__graph_dict:\n",
    "            for u in self.__graph_dict[v]:\n",
    "                for w in self.__graph_dict[v]:\n",
    "                    if u != w: \n",
    "                        triplet += 1\n",
    "                    if u != w and w in self.__graph_dict[u]:\n",
    "                        triangle += 1\n",
    "        return triangle / triplet\n",
    "    \n",
    "    \"\"\" Graph traversal \"\"\"\n",
    "    def components_BFS(self, vertices, comps, base_vertice):\n",
    "        if not vertices:\n",
    "            return\n",
    "        new_vertices = []\n",
    "        for v in vertices:\n",
    "            comps[v] = base_vertice\n",
    "            for u in self.__graph_dict[v]:\n",
    "                if comps[u] == u and u != base_vertice:\n",
    "                    new_vertices.append(u)\n",
    "        self.components_BFS(new_vertices, comps, base_vertice)\n",
    "    \n",
    "    def __generate_components(self):\n",
    "        \"\"\"Retuen a dictionnary of components representant\"\"\"\n",
    "        comps = {}\n",
    "        for u in self.__graph_dict:\n",
    "            comps[u] = u\n",
    "        for u in self.__graph_dict:\n",
    "            if comps[u] == u:\n",
    "                self.components_BFS([u], comps, u)\n",
    "        return comps\n",
    "    \n",
    "    def shortest_path_BFS(self, vertices, seen, d, goal):\n",
    "        if not vertices:\n",
    "            return math.inf\n",
    "        if goal in vertices:\n",
    "            return d\n",
    "        new_vertices = set()\n",
    "        for v in vertices:\n",
    "            seen.add(v)\n",
    "        for v in vertices:\n",
    "            for u in self.__graph_dict[v]:\n",
    "                if u not in seen:\n",
    "                    new_vertices.add(u)\n",
    "        return self.shortest_path_BFS(new_vertices, seen, d+1, goal)\n",
    "    \n",
    "    def shortest_path(self, s, t):\n",
    "        \"\"\"Return the shortest path between s and t\"\"\"\n",
    "        return self.shortest_path_BFS({s}, set(), 0, t)\n",
    "    \n",
    "    def diameter_BFS(self, vertices, seen, d):\n",
    "        if not vertices:\n",
    "            if d > 0:\n",
    "                return d-1\n",
    "            else:\n",
    "                return d\n",
    "        new_vertices = set()\n",
    "        for v in vertices:\n",
    "            seen.add(v)\n",
    "        for v in vertices:\n",
    "            for u in self.__graph_dict[v]:\n",
    "                if u not in seen:\n",
    "                    new_vertices.add(u)\n",
    "        return self.diameter_BFS(new_vertices, seen, d+1)\n",
    "    \n",
    "    def diameter(self):\n",
    "        diam = 0\n",
    "        for u in self.__graph_dict:\n",
    "            m = self.diameter_BFS({u}, set(), 0)\n",
    "            if (m > diam):\n",
    "                diam = m\n",
    "        return diam\n",
    "    \n",
    "    def diameter_component(self, u):\n",
    "        \"\"\" Return the diameter of the component containing vertex u \"\"\"\n",
    "        component = set()\n",
    "        comps = self.connected_components()\n",
    "        # First get the nodes of the component #\n",
    "        for v in self.__graph_dict:\n",
    "            if comps[v] == comps[u]:\n",
    "                component.add(v)\n",
    "        diam = 0\n",
    "        for v in component:\n",
    "            m = self.diameter_BFS({v}, set(), 0)\n",
    "            if (m > diam):\n",
    "                diam = m\n",
    "        return diam\n",
    "    \n",
    "    def biggest_component_diameter(self):\n",
    "        if not self.vertices(): return 0\n",
    "        \n",
    "        # First determine the biggest component #\n",
    "        comps = self.connected_components()\n",
    "        comps_size = {}\n",
    "        for u in comps.values():\n",
    "            comps_size[u] = 0\n",
    "        for v,u in comps.items():\n",
    "            comps_size[u] += 1\n",
    "        biggest = tuple(comps.values())[0]\n",
    "        max_size = comps_size[biggest]\n",
    "        for u in comps.values():\n",
    "            if comps_size[u] > max_size:\n",
    "                max_size = comps_size[u]\n",
    "                biggest = u\n",
    "        \n",
    "        # Return the diameter of the corresponding component\n",
    "        return self.diameter_component(biggest)\n",
    "        \n",
    "    def spanning_tree(self):\n",
    "        queue = []\n",
    "        tree = []\n",
    "        # Hack to get one key in the dict\n",
    "        for v in self.__graph_dict:\n",
    "            queue.append(v)\n",
    "            break\n",
    "        seen = [queue[-1]]\n",
    "\n",
    "        while queue:\n",
    "            u = queue.pop()\n",
    "            for v in self.__graph_dict[u]:\n",
    "                if not v in seen:\n",
    "                    tree.append(set([u,v]))\n",
    "                    seen.append(v)\n",
    "                    queue.append(v)\n",
    "        return tree\n",
    "    \n",
    "    def irregular_edge_count(self):\n",
    "        nb_loop = 0\n",
    "        nb_multi = 0\n",
    "        nb_edge = 0\n",
    "        for v in self.__graph_dict:\n",
    "            seen = []\n",
    "            for u in self.__graph_dict[v]:\n",
    "                nb_edge += 1\n",
    "                if u == v:\n",
    "                    nb_loop += 1\n",
    "                elif u in seen:\n",
    "                    nb_multi += 1\n",
    "                else:\n",
    "                    seen.append(u)\n",
    "        return (nb_loop + nb_multi // 2) / nb_edge * 2\n",
    "        \n",
    "    \"\"\" Static methods for defining classical graphs \"\"\"\n",
    "    @staticmethod\n",
    "    def clique(n):\n",
    "        d = {}\n",
    "        s = set(i+1 for i in range(n))\n",
    "        for i in range(n):\n",
    "            d[i+1] = s.difference(set({i+1}))\n",
    "        return Graph(d,\"$K_\"+str(n)+\"$\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def no_edges(n):\n",
    "        d = {}\n",
    "        for i in range(n):\n",
    "            d[i+1] = []\n",
    "        return Graph(d,\"$D_\"+str(n)+\"$\")\n",
    "\n",
    "    \"\"\" Importing from a text file \"\"\"\n",
    "    @staticmethod\n",
    "    def from_txt(file):\n",
    "        G = Graph()\n",
    "        lines = open(file).readlines()\n",
    "        for l in lines:\n",
    "            p = parse('{:d}\\t{:d}', l)\n",
    "            G.add_vertex(p[0])\n",
    "            G.add_vertex(p[1])\n",
    "            G.add_edge({p[0],p[1]})\n",
    "        return G\n",
    "    def print(self):\n",
    "        print(self.__graph_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Erdös-Rényi random graph model\n",
    "### Generating ER graphs\n",
    "We first implement the two variants that we are studying in this section:\n",
    "- the first one, in function `er_np`, returns a graph (drawn from set $G_{n,p}$) with the specified number of nodes $n$ and each couple of nodes is linked independently from the others with the given probability $p$;\n",
    "- the second one, in function `er_nm` returns a graph (drawn from set $G_{n,m}$) with the specified number of nodes $n$ and the exact given number of edges $m$ that are chosen uniformly among all possible edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def er_np(n, p):\n",
    "    graph_dict = {}\n",
    "    for i in range(n):\n",
    "        graph_dict[i] = []\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            rv = random.uniform(0,1)\n",
    "            if rv < p:\n",
    "                graph_dict[i].append(j)\n",
    "                graph_dict[j].append(i)  \n",
    "    return Graph(graph_dict)\n",
    "\n",
    "def er_nm(n, m):\n",
    "    graph_dict = {}\n",
    "    for i in range(n):\n",
    "        graph_dict[i] = []\n",
    "    # number of possible edges (*2 since it is easier to sample couple than pairs)\n",
    "    nb_possibility = n*n\n",
    "    count_left = m\n",
    "    if m > n*(n-1)/2 :\n",
    "        count_left = n*(n-1)/2\n",
    "    while count_left > 0:\n",
    "        # sample an index for the new edge\n",
    "        edge_ind = random.randrange(nb_possibility)\n",
    "        # get the two endpoints (part where it is easier for couple)\n",
    "        i = edge_ind // n\n",
    "        j = edge_ind % n\n",
    "        # handle loops and multiedges\n",
    "        if i == j :\n",
    "            continue \n",
    "        if j in graph_dict[i]:\n",
    "            continue\n",
    "        graph_dict[i].append(j)\n",
    "        graph_dict[j].append(i)\n",
    "        count_left -= 1\n",
    "    return Graph(graph_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of edges\n",
    "A graph drawn from set $G_{n,p}$ has an expected number of edges which is $$\\left\\lfloor p \\binom{n}{2} \\right\\rfloor.$$ We want to compare the number of edges obtained from graphs generated by our function `er_np` with the expected number of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_edge_number(n, p):\n",
    "    return math.floor(p * n*(n-1)/2)\n",
    "\n",
    "def binomial_coef(n, k):\n",
    "    return scipy.special.binom(n, k)\n",
    "\n",
    "def compare_edge_count(n, p):\n",
    "    \"\"\"Generate an er_np graph and compare its edges to an er_nm\"\"\"\n",
    "    m = expected_edge_number(n, p)\n",
    "    G_tmp = er_np(n, p)\n",
    "    nb_edges = len(G_tmp.edges())\n",
    "    if m == 0:\n",
    "        m = 1\n",
    "    return nb_edges / m\n",
    "\n",
    "def compare_edge_counts(n):\n",
    "    x = [ p for p in np.arange(1/n, 1.0, 1/n)]\n",
    "    got = [ compare_edge_count(n, p) for p in  np.arange(1/n, 1.0, 1/n)]\n",
    "    plt.plot(x, got, label='Ratio |E| / $\\\\left\\\\lfloor p \\\\binom{n}{2} \\\\right\\\\rfloor$')\n",
    "    plt.axhline(1., ls='--', c='red')\n",
    "    plt.xlabel('$p$')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "w = interact(compare_edge_counts\n",
    "            ,n=widgets.IntSlider(description='$n$', min=1, max=200, step=1, value=100, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By playing a bit with the previous plot, we can make several observations:\n",
    "- most of the big differences between the number of edges and the expected value happen when $p$ is small;\n",
    "- we rarely observe a ratio greater than $1.1$ and smaller than $0.9$;\n",
    "- when $p$ is not small (typically $p \\geq 0.3$), we rarely observe a ratio greater than $1.05$ and smaller than $0.95$.\n",
    "\n",
    "From these experiments, the variant drawing a graph in $G_{n,p}$ which is simpler than the other one seems to be a rather good approximation of drawing a graph with $\\left\\lfloor p \\binom{n}{2} \\right\\rfloor$ edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree distribution\n",
    "We are now interested in the **degree distribution** of graphs in $G_{n,p}$. We know that the theoretical distribution of degrees in a graph in $G_{n,p}$ is a binomial law with parameter $n-1$ and $p$.\n",
    "We first plot the result for *one* graph generated with the `er_np` function, and we plot the corresponding binomial law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree distribution\n",
    "n = 1000\n",
    "p = .5\n",
    "def degree_distr_er_np(n, p):\n",
    "    graph_tmp = er_np(n, p)\n",
    "    got = [0 for k in range(n)]\n",
    "    for x in graph_tmp.vertex_degrees().values():\n",
    "        got[x] += 1\n",
    "    for k in range(n):\n",
    "        got[k] /= n\n",
    "    plt.plot(got, label='Measured')\n",
    "    expected = [binomial_coef(n-1, k) * math.pow(p, k) * math.pow(1-p, n-1-k) for k in range(n)]\n",
    "    plt.plot(expected, 'r--', label='$\\\\mathcal{B}(n-1,p)$')\n",
    "    plt.xlim(n*p - 0.5*n*p*(1-p), n*p + 0.5*n*p*(1-p))\n",
    "    plt.xlabel('Degree')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "w = interact(\n",
    "        degree_distr_er_np\n",
    "       ,n=widgets.IntSlider(description='$n$',min=100,max=1000,step=10,value=700,continuous_update=False)\n",
    "       ,p=widgets.FloatSlider(description='$p$',min=0.1,max=0.9,step=0.01,value=0.5,continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the degree distribution in graphs of $G_{n,p}$ *roughly* follows a binomial law. In particular, the values for which the probability are very close to 0 are the same, and when the probability is non-zero then it has very roughly the same shape. \n",
    "\n",
    "In order to smooth a little bit this curve, we make another plot where we take an average degree distribution over a small number of graphs.\n",
    "\n",
    "**Remark:** in nearly all the following experiments, we take an average over a small number of graphs (less than 20) in order to smooth the results. Since this number is small compared to the total number of possible graphs, it gives us a trend of what happens for most graph, as it is *very* unlikely that we observe a case where there have been a \"mean effect\" that entirely flaws the result of the experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 20\n",
    "\n",
    "def average_edge_distr_er_np(n, p):\n",
    "    got = [0 for k in range(n)]\n",
    "    for i in range(nb):\n",
    "        graph_tmp = er_np(n, p)\n",
    "        for x in graph_tmp.vertex_degrees().values():\n",
    "            got[x] += 1\n",
    "    for k in range(n):\n",
    "        got[k] /= n * nb\n",
    "    plt.plot(got, label='Average measured')\n",
    "    expected = [binomial_coef(n-1, k) * math.pow(p, k) * math.pow(1-p, n-1-k) for k in range(n)]\n",
    "    plt.plot(expected, 'r--', label='$\\\\mathcal{B}(n-1,p)$')\n",
    "    plt.xlim(n*p - 0.3*n*p*(1-p), n*p + 0.3*n*p*(1-p))\n",
    "    plt.xlabel('Degree')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "w = interact(\n",
    "        average_edge_distr_er_np\n",
    "       ,n=widgets.IntSlider(description='$n$',min=100,max=1000,step=10,value=700,continuous_update=False)\n",
    "       ,p=widgets.FloatSlider(description='$p$',min=0.1,max=0.9,step=0.01,value=0.5,continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very clear on this new plot that the degree distribution of graphs drawn from $G_{n,p}$ is close to a  binomial law with parameter $n-1$ and $p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected components\n",
    "We are now interested in the **connected components** of graphs in $G_{n,p}$. We implement a function `biggest_comp_size` that returns the size of the biggest component of a given graph, and `two_biggest_comp_size` that also returns the size of the second biggest component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def biggest_comp_size(G):\n",
    "    comps = G.connected_components()\n",
    "    # First determine the biggest component #\n",
    "    comps_size = {}\n",
    "    for u in comps.values():\n",
    "        comps_size[u] = 0\n",
    "    for v,u in comps.items():\n",
    "        comps_size[u] += 1\n",
    "    biggest = tuple(comps.values())[0]\n",
    "    max_size = comps_size[biggest]\n",
    "    for u in comps.values():\n",
    "        if comps_size[u] > max_size:\n",
    "            max_size = comps_size[u]\n",
    "            biggest = u\n",
    "    return max_size\n",
    "def two_biggest_comp_size(G):\n",
    "    comps = G.connected_components()\n",
    "    # First determine the biggest component #\n",
    "    comps_size = {}\n",
    "    for u in comps.values():\n",
    "        comps_size[u] = 0\n",
    "    for v,u in comps.items():\n",
    "        comps_size[u] += 1\n",
    "    biggest = tuple(comps.values())[0]\n",
    "    max_size = comps_size[biggest]\n",
    "    for u in comps.values():\n",
    "        if comps_size[u] > max_size:\n",
    "            max_size = comps_size[u]\n",
    "            biggest = u\n",
    "    sec_size = 0\n",
    "    for u in comps.values():\n",
    "        if u == biggest:\n",
    "            continue\n",
    "        else:\n",
    "            if comps_size[u] > sec_size:\n",
    "                sec_size = comps_size[u]\n",
    "    return max_size, sec_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next plot, we fix some $n$ and see how the size of the biggest component, of the second biggest component, and the number of isolated vertices (i.e. vertices with degree 0) evolve with $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_average = 10\n",
    "\n",
    "def connected_components_er_np(n):\n",
    "    end_point = math.ceil(math.log(n,2)+1)\n",
    "    step = 20*end_point\n",
    "\n",
    "    bcomp_size = [0 for i in range(step)]\n",
    "    sec_size = [0 for i in range(step)]\n",
    "    isolated_size = [0 for i in range(step)]\n",
    "    x = [0 for i in range(step)]\n",
    "    for t in range(nb_average):\n",
    "        for i in range(step):\n",
    "            p = end_point * i / (step * n) \n",
    "            x[i] = n * p\n",
    "            G = er_np(n, p)\n",
    "            bcomp, sec = two_biggest_comp_size(G)\n",
    "            bcomp_size[i] += bcomp\n",
    "            sec_size[i] += sec\n",
    "            isolated_size[i] += len(G.find_isolated_vertices())\n",
    "    bcomp_size = [x / nb_average for x in bcomp_size]\n",
    "    sec_size = [x / nb_average for x in sec_size]\n",
    "    isolated_size = [x / nb_average for x in isolated_size]\n",
    "\n",
    "\n",
    "    plt.plot(x, bcomp_size, label='Biggest component')\n",
    "    plt.plot(x, sec_size, label='Second biggest component')\n",
    "    plt.plot(x, isolated_size, label='Isolated vertices')\n",
    "\n",
    "    #plt.axhline(math.pow(n, 2/3), ls = '--', c='red')\n",
    "    #plt.text(0., math.pow(n, 2/3)+0.01 * n, '$n^{2/3}$')\n",
    "\n",
    "    #plt.axvline(1, ls = '--', c = 'red')\n",
    "    plt.axhline(math.log(n,2), ls='--', c='red')\n",
    "    plt.text(math.log(n,2)-0.1*math.log(n,2), math.log(n,2)+0.01 * n, '$\\log\\ n$')\n",
    "\n",
    "    plt.axvline(math.log(n, 2), ls = '--', c = 'red')\n",
    "    plt.text(math.log(n,2) - 0.03 * end_point ,n/2,'$\\log\\ n$',rotation=90)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('$np$')\n",
    "    plt.show()\n",
    "    \n",
    "w = interact(\n",
    "        connected_components_er_np\n",
    "        ,n=widgets.IntSlider(description='$n$',min=10,max=200,value=100,continuous_update=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the value of $n$, we can remark an important fact: when $np$ is greater than $\\log n$, the size of the biggest component is almost equal to $n$, i.e. the graph is almost surely connected (the difference is less than 1 when we take the average). We cannot draw a lot of other conclusions from this graph since we need to draw the values as a function of $n$ and here we fixed $n$.\n",
    "\n",
    "In the next plot, we fix some constant $c>0$ and we draw the size of the biggest component and when it is interesting the size of the second biggest component as a function of $n$, the probability parameter for the random graph generation being $$p=\\frac{c}{n}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = c/n with different values of c and let n go big\n",
    "nb_average = 20\n",
    "\n",
    "def connected_components_er_np_bis(c, n_max):\n",
    "    # Determine minimum n such that c/n < 1\n",
    "    n_min = math.ceil(c)\n",
    "    bcomp_size = [0 for i in range(n_min,n_max+1)]\n",
    "    if c > 1.:\n",
    "        sec_size = [0 for i in range(n_min,n_max+1)]\n",
    "    x = [i for i in range(n_min,n_max+1)]\n",
    "    for t in range(nb_average):\n",
    "        for n in range(n_min,n_max+1):\n",
    "            G = er_np(n, c/n)\n",
    "            if c > 1.:\n",
    "                bcomp, sec = two_biggest_comp_size(G)\n",
    "                bcomp_size[n-n_min] += bcomp\n",
    "                sec_size[n-n_min] += sec\n",
    "            else:\n",
    "                bcomp = biggest_comp_size(G)\n",
    "                bcomp_size[n-n_min] += bcomp\n",
    "    bcomp_size = [x / nb_average for x in bcomp_size]\n",
    "    if c > 1.:\n",
    "        sec_size = [x / nb_average for x in sec_size]\n",
    "    \n",
    "    plt.plot(x, bcomp_size, label='Biggest component')\n",
    "    if c > 1.:\n",
    "        plt.plot(x, sec_size, label='Second biggest')\n",
    "    \n",
    "    # Draw hypothesis according to constant c\n",
    "    if c == 1.:\n",
    "        hyp = [math.pow(n,2/3) for n in range(n_min, n_max+1)]\n",
    "        plt.plot(x, hyp, 'r--', label='$n^{2/3}$')\n",
    "    elif c<1.:\n",
    "        # Doing a logarithm fit for the biggest component when c>1\n",
    "        log_fit = np.polyfit(np.log(x), bcomp_size, 1)\n",
    "        hyp = [log_fit[0]*math.log(n)+log_fit[1] for n in range(n_min, n_max+1)]\n",
    "        op = ''\n",
    "        if (log_fit[1] > 0):\n",
    "            op = '+'\n",
    "        plt.plot(x, hyp, 'r--', label='$'+str(round(log_fit[0],2))+'\\ \\ln\\ x'+op+str(round(log_fit[1],2))+'$')\n",
    "    else:\n",
    "        # Doing a logarithm fit for the biggest component when c>1\n",
    "        log_fit = np.polyfit(np.log(x), sec_size, 1)\n",
    "        hyp = [log_fit[0]*math.log(n)+log_fit[1] for n in range(n_min, n_max+1)]\n",
    "        op = ''\n",
    "        if (log_fit[1] > 0):\n",
    "            op = '+'\n",
    "        plt.plot(x, hyp, 'r--', label='$'+str(round(log_fit[0],2))+'\\ \\ln\\ x'+op+str(round(log_fit[1],2))+'$')\n",
    "        lin_fit = np.polyfit(x, bcomp_size, 1)\n",
    "        hyp2 = [lin_fit[0] * n + lin_fit[1] for n in range(n_min, n_max+1)]\n",
    "        op = ''\n",
    "        if (lin_fit[1] > 0):\n",
    "            op = '+'\n",
    "        plt.plot(x, hyp2, 'g--', label='$'+str(round(lin_fit[0],2))+'x'+op+str(round(lin_fit[1],2))+'$')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('$n$')\n",
    "    plt.show()\n",
    "    \n",
    "w = interact(\n",
    "        connected_components_er_np_bis\n",
    "       ,n_max=widgets.IntSlider(description='$n_\\\\text{max}$',min=10,max=200,value=130,step=5,continuous_update=False)\n",
    "       ,c=widgets.FloatSlider(description='$c$',min=0.1,max=2.,value=1.,step=0.05,continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three cases that we observe in this plot:\n",
    "- when $c=1$: the biggest component has a size which is close to the function $n^{2/3}$;\n",
    "- when $c<1$: the biggest component has a size which is logarithmic as a function of $n$;\n",
    "- when $c>1$: the biggest component has a size which is linear (i.e. there is a positive fraction of the nodes in the component) and the second biggest component has logarithmic size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that if $np > \\log n$, the graph is almost surely connected. In the next plot we want to verify that if $np < \\log n$, then there is almost surely at least one isolated vertex.\n",
    "\n",
    "We choose a constant $c < 1$ and plot the fraction of graphs among a small number of tests that do not contain any isolated vertices, using $p= c \\frac{\\log n}{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = c log(n)/n with different values of c and let n go big\n",
    "nb_tests = 10\n",
    "\n",
    "def connected_components_er_np_bis(c, n_max):\n",
    "    # Determine minimum n such that c log(n)/n < 1\n",
    "    n_min = 200\n",
    "    while c* math.log(n_min, 2) / n_min > 1 and n_min <= n_max:\n",
    "        n_min += 1\n",
    "    \n",
    "    if n_min == n_max:\n",
    "        return\n",
    "    \n",
    "    nb_times_non_isolated = [0 for i in range(n_min,n_max+1)]\n",
    "    x = [i for i in range(n_min,n_max+1)]\n",
    "    for t in range(nb_tests):\n",
    "        for n in range(n_min,n_max+1):\n",
    "            G = er_np(n, c * math.log(n, 2)/n)\n",
    "            if not G.has_isolated_vertices():\n",
    "                nb_times_non_isolated[n-n_min] += 1\n",
    "    nb_times_non_isolated = [x / nb_tests for x in nb_times_non_isolated]\n",
    "    \n",
    "    plt.plot(x, nb_times_non_isolated, 'ro', label='Fraction of times where no isolated vertices')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('$n$')\n",
    "    plt.show()\n",
    "    \n",
    "w = interact(\n",
    "        connected_components_er_np_bis\n",
    "       ,n_max=widgets.IntSlider(description='$n_\\\\text{max}$',min=200,max=400,value=300,step=5,continuous_update=False)\n",
    "       ,c=widgets.FloatSlider(description='$c$',min=0.1,max=1.,value=0.5,step=0.05,continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for $c = 0.5$ and $n \\geq 200$ the fraction of generated graphs with no isolated vertices is close to zero. However when $c$ is closer to 1, it is difficult to observe this property as we need to use larger graphs and the computation time is too large.\n",
    "\n",
    "The results in this section correspond to the well-known characteristics of ER graphs proved by Erdös and Rényi in 1960."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The configuration model of random networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we see another method to construct random graphs with a given target degree sequence.\n",
    "In particular, we define below regular and lognormal degree sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def degree_sequence_regular(n, k):\n",
    "    return [k for i in range(n)]\n",
    "\n",
    "def degree_sequence_lognormal(n, mu, sigma):\n",
    "    return [max(0, math.floor(np.random.lognormal(mu, sigma))) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct such graphs, we consider on each vertice a number of \"stubs\" given by the degree sequence.\n",
    "These stubs represent half edges that we are going to connect randomly two by two. Here we alow the construction of loop and multi-edges in the final graphs. Additionnaly, one stub may not find a pair depending on the parity of the number of stubs.\n",
    "\n",
    "The following function connect the stubs in a random way by choosing two vertices randomly that still have some stubs and then connect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def configure_sequence(seq):\n",
    "    non_zero_idx = [ i for i in range(len(seq))]\n",
    "    \n",
    "    G = {}\n",
    "    for i in range(len(seq)):\n",
    "        G[i] = []\n",
    "    \n",
    "    while len(non_zero_idx) > 0:\n",
    "        id1 = non_zero_idx[random.randrange(len(non_zero_idx))] #get first stubs\n",
    "        has_removed_1 = False\n",
    "        if seq[id1] > 1:\n",
    "            seq[id1] -= 1\n",
    "        elif seq[id1] == 1:\n",
    "            seq[id1] = 0\n",
    "            has_removed_1 = True\n",
    "            non_zero_idx.remove(id1)\n",
    "        else:\n",
    "            has_removed_1 = True\n",
    "            non_zero_idx.remove(id1)\n",
    "        \n",
    "        if len(non_zero_idx) == 0:\n",
    "            continue\n",
    "        \n",
    "        id2 = non_zero_idx[random.randrange(len(non_zero_idx))] #get 2nd stubs\n",
    "        if seq[id2] > 1:\n",
    "            seq[id2] -= 1\n",
    "        elif seq[id2] == 1:\n",
    "            seq[id2] = 0\n",
    "            non_zero_idx.remove(id2)\n",
    "        elif not has_removed_1 and len(non_zero_idx) == 1:\n",
    "            break\n",
    "        else:\n",
    "            if has_removed_1:\n",
    "                non_zero_idx.append(id1)\n",
    "            seq[id1] += 1\n",
    "            continue\n",
    "            \n",
    "        G[id1].append(id2)\n",
    "        G[id2].append(id1)\n",
    "        \n",
    "    return Graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the next plot, we show that for a graph with $n$ vertices and $k$-regular target degree sequence, only a small fraction of vertices actually get their $k$ neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_sequence_regular(n, k):\n",
    "    distrib = configure_sequence(degree_sequence_regular(n, k)).degree_distrib_simple()\n",
    "    x = sorted([d for d in distrib])\n",
    "    distrib_values = [distrib[d] for d in x]\n",
    "    plt.plot(x, distrib_values, label='Degree distribution')\n",
    "    plt.xlabel('Degree')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "n_widget = widgets.IntSlider(description='$n$',min=10,max=400,value=200,step=5,continuous_update=False)\n",
    "k_widget = widgets.IntSlider(description='$k$',min=10,max=200,value=60,step=5,continuous_update=False)\n",
    "def update_k_range(*args):\n",
    "    k_widget.max = n_widget.value\n",
    "n_widget.observe(update_k_range, 'value')\n",
    "\n",
    "w = interact(plot_degree_sequence_regular, n=n_widget, k=k_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the fraction of irregular edges (self loop and multi edges) among the number of edges of the graphs using a $k$-regular degree sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 10\n",
    "n_max = 500\n",
    "step = 5\n",
    "\n",
    "def plot_irregular_edges_regular(k):\n",
    "    nb_sample = (n_max - n_min) // step\n",
    "    irregular = [ 0 for i in range(nb_sample)]\n",
    "    x = [ 0 for i in range(nb_sample)]\n",
    "    for i in range(nb_sample):\n",
    "        x[i] = n_min + i * step\n",
    "        irregular[i] = configure_sequence(degree_sequence_regular(x[i], k)).irregular_edge_count()\n",
    "    \n",
    "    # Determine value for which all values are less than 0.1 after it\n",
    "    n_01 = nb_sample-1\n",
    "    while n_01 >= 0 and irregular[n_01] <= 0.1:\n",
    "        n_01 -= 1\n",
    "    \n",
    "    plt.plot(x, irregular, label='Fraction of irregular edges')\n",
    "    plt.axhline(0.1, ls='--', c='red')\n",
    "    if n_01 < nb_sample-10:\n",
    "        plt.axvline(n_min + n_01*step, ls='--', c='red')\n",
    "    plt.xlabel('$n$')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "w = interact(plot_irregular_edges_regular, k=widgets.IntSlider(description='$k$',min=0,max=100,value=20,step=5,continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the fraction of irregular edges tends to 0 with $n$. Moreover the larger $k$ is, the slower this fraction tends to 0 (typically, this fraction is less than $0.1$ for $n \\approx 150$ when $k=20$, and for $n \\approx 400$ when $k=80$).\n",
    "\n",
    "\n",
    "Next, we plot the fraction of irregular edges (self loop and multi edges) among the number of edges of the graphs using a lognormal degree sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min = 10\n",
    "n_max = 700\n",
    "step = 2\n",
    "mu = 2\n",
    "sigma = 1\n",
    "\n",
    "def plot_irregular_edges_lognormal(mu, sigma):\n",
    "    nb_sample = (n_max - n_min) // step\n",
    "    irregular = [ 0 for i in range(nb_sample)]\n",
    "    x = [ 0 for i in range(nb_sample)]\n",
    "    for i in range(nb_sample):\n",
    "        x[i] = n_min + i * step\n",
    "        irregular[i] = configure_sequence(degree_sequence_lognormal(x[i], mu, sigma)).irregular_edge_count()\n",
    "    \n",
    "    # Determine value for which all values are less than 0.1 after it\n",
    "    n_01 = nb_sample-1\n",
    "    while n_01 >= 0 and irregular[n_01] <= 0.1:\n",
    "        n_01 -= 1\n",
    "    \n",
    "    plt.plot(x, irregular, label='Fraction of irregular edges')\n",
    "    plt.axhline(0.1, ls='--', c='red')\n",
    "    if n_01 < nb_sample-10:\n",
    "        plt.axvline(n_min + n_01*step, ls='--', c='red')\n",
    "    plt.legend()\n",
    "    plt.xlabel('$n$')\n",
    "    plt.show()\n",
    "\n",
    "w = interact(plot_irregular_edges_lognormal, mu=widgets.FloatSlider(description='$\\mu$',min=0.1,max=2.,value=1,step=0.05,continuous_update=False),\n",
    "             sigma=widgets.FloatSlider(description='$\\sigma$',min=0.1,max=2.,value=0.7,step=0.05,continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous graph also shows that the fraction of irregular edges tends to 0 as $n$ grows. We can see that the parameter $\\sigma$ is very influencial on the rapidity of decrease of the fraction: the larger it is, the slower it tends to 0. Typically, for $\\sigma = 0.7$ the fraction is less than $0.1$ from $n \\approx 300$ and for $\\sigma = 0.85$ the fraction is less than $0.1$ from $n \\approx 600$.\n",
    "\n",
    "These two last experiments shows that the configuration model of random networks generates *almost simple* graphs when $n$ is large, which means that by deleting self-loops and multiple edges we get graphs which have **very close** degree sequence to the target degree sequence when $n$ is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
